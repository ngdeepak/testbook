{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improving-navigator",
   "metadata": {},
   "source": [
    "```{figure} ../images/banner.png\n",
    "---\n",
    "align: center\n",
    "name: banner\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-destiny",
   "metadata": {},
   "source": [
    "# Chapter 2 : DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-hopkins",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-perth",
   "metadata": {},
   "source": [
    "- understand the syntax to create dataframe\n",
    "- create spark dataframe from different data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-extra",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "\n",
    "- [1. What is spark dataframe](#1)\n",
    "- [2. Creating a spark dataframe](#2)\n",
    "    - [2a. from RDD](#3)\n",
    "    - [2b. from List](#4)\n",
    "    - [2c. from pandas dataframe](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-invention",
   "metadata": {},
   "source": [
    "If you do not want to go through the entire chapter, \n",
    "## Click on one of the image below that achieves the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-discount",
   "metadata": {},
   "source": [
    "A | B\n",
    "- | - \n",
    "[![alt](img/chapter2/rdd_dataframe.png)](#23) | [![alt](img/chapter2/list_dataframe.png)](#33)\n",
    "[![alt](img/chapter2/pd_spark.png)](#23) | \n",
    "<hr></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-image",
   "metadata": {},
   "source": [
    "[![alt text](img/chapter2/rdd_dataframe.png \"Title\")](#23)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-minute",
   "metadata": {},
   "source": [
    "[![alt text](img/chapter2/list_dataframe.png \"Title\")](#23)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-campus",
   "metadata": {},
   "source": [
    "[![alt text](img/chapter2/pd_spark.png \"Title\")](#23)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-elite",
   "metadata": {},
   "source": [
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-season",
   "metadata": {},
   "source": [
    "## 1. What is spark dataframe?\n",
    "A DataFrame simply represents a table of data with rows and columns. A simple analogy would be a spreadsheet with named columns.\n",
    "\n",
    "Spark Data Frame is a distributed collection of data organized into named columns. It can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDD, Lists, Pandas data frame. \n",
    "\n",
    "\n",
    "```{figure} img/chapter2/spark_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-characterization",
   "metadata": {},
   "source": [
    "###  2. Creating a spark dataframe \n",
    "\n",
    "Lets first understand the syntax\n",
    "\n",
    "```{admonition} Syntax\n",
    "<b>createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)</b>\n",
    "\n",
    "<b>Parameters</b>:\n",
    "\n",
    "data – RDD,list, or pandas.DataFrame.\n",
    "\n",
    "schema – a pyspark.sql.types.DataType or a datatype string or a list of column names, default is None. \n",
    "\n",
    "samplingRatio – the sample ratio of rows used for inferring\n",
    "\n",
    "verifySchema – verify data types of every row against schema.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-spokesman",
   "metadata": {},
   "source": [
    "<a id='33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-burst",
   "metadata": {},
   "source": [
    "## 2a. from RDD\n",
    "\n",
    "\n",
    "```{figure} img/chapter2/rdd_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "<b>What is RDD?</b>\n",
    "\n",
    "Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. \n",
    "\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. \n",
    "\n",
    "RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. \n",
    "\n",
    "Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n",
    "\n",
    "<b>Creating RDD :</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "victorian-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "rdd_spark = spark.sparkContext.parallelize([('John', 'Seattle', 60, True, 1.7, '1960-01-01'),\n",
    " ('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'),\n",
    " ('Mike', 'New York', 40, True, 1.65, '1980-01-01')]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patent-technology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'Seattle', 60, True, 1.7, '1960-01-01'), ('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), ('Mike', 'New York', 40, True, 1.65, '1980-01-01')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-dress",
   "metadata": {},
   "source": [
    "<b>Creating a spark dataframe:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "charged-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+-----+----+----------+\n",
      "|  _1|       _2| _3|   _4|  _5|        _6|\n",
      "+----+---------+---+-----+----+----------+\n",
      "|John|  Seattle| 60| true| 1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30|false| 1.8|1990-01-01|\n",
      "|Mike| New York| 40| true|1.65|1980-01-01|\n",
      "+----+---------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(rdd_spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-provision",
   "metadata": {},
   "source": [
    "## 2b. from List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-centre",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter2/list_dataframe.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automatic-affair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+-----+----+----------+\n",
      "|  _1|       _2| _3|   _4|  _5|        _6|\n",
      "+----+---------+---+-----+----+----------+\n",
      "|John|  Seattle| 60| true| 1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30|false| 1.8|1990-01-01|\n",
      "|Mike| New York| 40| true|1.65|1980-01-01|\n",
      "+----+---------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-sapphire",
   "metadata": {},
   "source": [
    "<a id='23'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-football",
   "metadata": {},
   "source": [
    "## 2c. from pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-possibility",
   "metadata": {},
   "source": [
    "\n",
    "```{figure} img/chapter2/pd_spark.png\n",
    "---\n",
    "align: center\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-horizontal",
   "metadata": {},
   "source": [
    "<b>Input: pandas dataframe</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-mixture",
   "metadata": {},
   "source": [
    "<b>Creating pandas dataframe</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amateur-thirty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1960-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tony</td>\n",
       "      <td>Cupertino</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1990-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike</td>\n",
       "      <td>New York</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1980-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0          1   2      3     4           5\n",
       "0  John    Seattle  60   True  1.70  1960-01-01\n",
       "1  Tony  Cupertino  30  False  1.80  1990-01-01\n",
       "2  Mike   New York  40   True  1.65  1980-01-01"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.DataFrame([('John', 'Seattle', 60, True, 1.7, '1960-01-01'), \n",
    "('Tony', 'Cupertino', 30, False, 1.8, '1990-01-01'), \n",
    "('Mike', 'New York', 40, True, 1.65, '1980-01-01')])\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-specific",
   "metadata": {},
   "source": [
    "<b>Output: spark dataframe</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minus-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---+-----+----+----------+\n",
      "|   0|        1|  2|    3|   4|         5|\n",
      "+----+---------+---+-----+----+----------+\n",
      "|John|  Seattle| 60| true| 1.7|1960-01-01|\n",
      "|Tony|Cupertino| 30|false| 1.8|1990-01-01|\n",
      "|Mike| New York| 40| true|1.65|1980-01-01|\n",
      "+----+---------+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_pd).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-grace",
   "metadata": {},
   "source": [
    "## .  &emsp; 3a. test1\n",
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-butter",
   "metadata": {},
   "source": [
    "##  &emsp;  5. &emsp; &emsp; test2\n",
    "<a id='5'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
