
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chapter 3 : Data Sources &#8212; Spark Data Frame Programming for Modern Data Engineering</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://github.com/ngdeepak/testbook/chapters/chapter3-data-sources.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 4 : Data Types &amp; Schema" href="chapter4-data-types-schema.html" />
    <link rel="prev" title="Chapter 2 : DataFrames" href="chapter2-dataframe.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://github.com/ngdeepak/testbook/chapters/chapter3-data-sources.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Chapter 3 : Data Sources" />
<meta property="og:description" content="Chapter 3 : Data Sources  Chapter Learning Objectives  Various data sources &amp; file formats.  General methods to read the data into spark dataframe.  General met" />
<meta property="og:image"       content="https://github.com/ngdeepak/testbook/_static/sparklogo.png" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/sparklogo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Spark Data Frame Programming for Modern Data Engineering</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Author.html">
   Jupiter Book
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter1-spark-basics.html">
   Chapter 1: Spark Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter2-dataframe.html">
   Chapter 2 : DataFrames
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Chapter 3 : Data Sources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter4-data-types-schema.html">
   Chapter 4 : Data Types &amp; Schema
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter5-string-column.html">
   Chapter 5 : String Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter6-number-column.html">
   Chapter 6 : Number Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter7-date-column.html">
   Chapter 7 : Date Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter8-array-column.html">
   Chapter 8 : Array Columns
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter9-map-column.html">
   Chapter 9 : Map Column
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter10-struct-column.html">
   Chapter 10 : Struct Column
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter11-json-string-column.html">
   Chapter 11 : JSON Column
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter12-null-nan-column.html">
   Chapter 12 : Null &amp; NaN Column
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter13-user-defined-function.html">
   Chapter 13 : User Defined Functions(UDFs)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter14-pandas-with-arrow.html">
   Chapter 14 : Pandas User Defined Function
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter15-aggregate-operations.html">
   Chapter 15 : Aggregate Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter16-join-operations.html">
   Chapter 16 : Join operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter17-window-operations.html">
   Chapter 17 : Window operations
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapters/chapter3-data-sources.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ngdeepak/testbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ngdeepak/testbook/issues/new?title=Issue%20on%20page%20%2Fchapters/chapter3-data-sources.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/ngdeepak/testbook/edit/master/chapters/chapter3-data-sources.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ngdeepak/testbook/master?urlpath=tree/chapters/chapter3-data-sources.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-learning-objectives">
   Chapter Learning Objectives
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-outline">
   Chapter Outline
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visual-outline">
   Visual Outline
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#various-data-sources">
   1. Various data sources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-text-file">
     1a. Text file
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-csv-file">
     1b. CSV file
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-json-file">
     1c.JSON file
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-parquet-file">
     1d. Parquet file
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-orc-file">
     1e. ORC file
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-avro-file">
     1e. AVRO file
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-whole-binary-file">
     1f. Whole Binary file
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading-writing-data-from-various-data-sources">
   2. Reading &amp; Writing data from various data sources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-reading-from-text-file-into-spark-data-frame">
     2a. Reading from text file into spark data frame
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-the-spark-data-frame-content-into-a-text-file">
       Saving the spark data frame content into a text file
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#b-reading-from-csv-file-into-spark-data-frame">
     2b. Reading from CSV file into spark data frame
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-the-spark-data-frame-content-into-a-csv-file">
       Saving the spark data frame content into a CSV file
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#c-reading-from-json-file-into-spark-data-frame">
     2c. Reading from JSON file into spark data frame
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-the-spark-data-frame-content-into-a-json-file">
       Saving the spark data frame content into a JSON  file
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-reading-from-parquet-file-into-spark-data-frame">
     2d. Reading from Parquet file into spark data frame
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-the-spark-data-frame-content-into-a-parquet-file">
       Saving the spark data frame content into a Parquet  file
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#e-reading-from-an-orc-file-into-spark-data-frame">
     2e. Reading from an ORC file into spark data frame
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-the-spark-data-frame-content-into-an-orc-file">
       Saving the spark data frame content into an ORC  file
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-reading-from-an-avro-file-into-spark-data-frame">
     2f. Reading from an AVRO file into spark data frame
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#saving-the-spark-data-frame-content-into-an-avro-file">
       Saving the spark data frame content into an AVRO  file
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#g-reading-from-an-whole-binary-file-into-spark-data-frame">
     2g. Reading from an Whole Binary file into spark data frame
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="figure align-center" id="banner">
<img alt="../_images/banner.png" src="../_images/banner.png" />
</div>
<style>body {text-align: justify}</style><div class="section" id="chapter-3-data-sources">
<h1>Chapter 3 : Data Sources<a class="headerlink" href="#chapter-3-data-sources" title="Permalink to this headline">¶</a></h1>
<div class="section" id="chapter-learning-objectives">
<h2>Chapter Learning Objectives<a class="headerlink" href="#chapter-learning-objectives" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Various data sources &amp; file formats.</p></li>
<li><p>General methods to read the data into spark dataframe.</p></li>
<li><p>General methods to write the spark dataframe into an external data source/file.</p></li>
</ul>
</div>
<div class="section" id="chapter-outline">
<h2>Chapter Outline<a class="headerlink" href="#chapter-outline" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#1">1. Various data sources</a></p>
<ul>
<li><p><a class="reference external" href="#2">1a. Text file</a></p></li>
<li><p><a class="reference external" href="#3">1b. CSV file</a></p></li>
<li><p><a class="reference external" href="#4">1c. JSON file</a></p></li>
<li><p><a class="reference external" href="#5">1d. Parquet file</a></p></li>
<li><p><a class="reference external" href="#6">1e. ORC file</a></p></li>
<li><p><a class="reference external" href="#7">1f. AVRO file</a></p></li>
<li><p><a class="reference external" href="#8">1g. Whole Binary file</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#9">2. Reading &amp; Writing data from various data sources</a></p>
<ul>
<li><p><a class="reference external" href="#10">2a. from text file</a></p></li>
<li><p><a class="reference external" href="#11">2b. from CSV file</a></p></li>
<li><p><a class="reference external" href="#12">2c. from JSON file</a></p></li>
<li><p><a class="reference external" href="#13">2d. from Parquet file</a></p></li>
<li><p><a class="reference external" href="#14">2e. from ORC file</a></p></li>
<li><p><a class="reference external" href="#15">2f. from AVRO file</a></p></li>
<li><p><a class="reference external" href="#16">2g. from whole Binary file</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="visual-outline">
<h2>Visual Outline<a class="headerlink" href="#visual-outline" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center">
<img alt="../_images/datasources.png" src="../_images/datasources.png" />
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:right head"><p>click on</p></th>
<th class="text-align:left head"><p>any image</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:right"><p><a class="reference external" href="#2"><img alt="alt" src="../_images/1a1.png" /></a></p></td>
<td class="text-align:left"><p><a class="reference external" href="#3"><img alt="alt" src="../_images/1b1.png" /></a></p></td>
</tr>
<tr class="row-odd"><td class="text-align:right"><p><a class="reference external" href="#4"><img alt="alt" src="../_images/1c.png" /></a></p></td>
<td class="text-align:left"><p><a class="reference external" href="#5"><img alt="alt" src="../_images/1d.png" /></a></p></td>
</tr>
<tr class="row-even"><td class="text-align:right"><p><a class="reference external" href="#6"><img alt="alt" src="../_images/1e.png" /></a></p></td>
<td class="text-align:left"><p><a class="reference external" href="#7"><img alt="alt" src="../_images/1f.png" /></a></p></td>
</tr>
<tr class="row-odd"><td class="text-align:right"><p><a class="reference external" href="#8"><img alt="alt" src="../_images/1g.png" /></a></p></td>
<td class="text-align:left"><p></p></td>
</tr>
</tbody>
</table>
<p><a id='1'></a></p>
</div>
<div class="section" id="various-data-sources">
<h2>1. Various data sources<a class="headerlink" href="#various-data-sources" title="Permalink to this headline">¶</a></h2>
<p>As a general computing engine, Spark can process data from various data management/storage systems, including HDFS, Hive, Cassandra and Kafka. For flexibility and high throughput, Spark defines the Data Source API, which is an abstraction of the storage layer. The Data Source API has two requirements.</p>
<ol class="simple">
<li><p>Generality: support reading/writing most data management/storage systems.</p></li>
<li><p>Flexibility: customize and optimize the read and write paths for different systems based on their capabilities.</p></li>
</ol>
<div class="figure align-center">
<img alt="../_images/datasources.png" src="../_images/datasources.png" />
</div>
<p><a id='2'></a></p>
<div class="section" id="a-text-file">
<h3>1a. Text file<a class="headerlink" href="#a-text-file" title="Permalink to this headline">¶</a></h3>
<p>A text file is a kind of computer file that is structured as a sequence of lines of electronic text.
Because of their simplicity, text files are commonly used for storage of information. They avoid some of the problems encountered with other file formats, such as  padding bytes, or differences in the number of bytes in a machine word. Further, when data corruption occurs in a text file, it is often easier to recover and continue processing the remaining contents.
A disadvantage of text files is that they usually have a low entropy, meaning that the information occupies more storage than is strictly necessary.</p>
<p>A simple text file may need no additional metadata (other than knowledge of its character set) to assist the reader in interpretation.</p>
<p><a id='3'></a></p>
</div>
<div class="section" id="b-csv-file">
<h3>1b. CSV file<a class="headerlink" href="#b-csv-file" title="Permalink to this headline">¶</a></h3>
<p>A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields.</p>
<p><a id='4'></a></p>
</div>
<div class="section" id="c-json-file">
<h3>1c.JSON file<a class="headerlink" href="#c-json-file" title="Permalink to this headline">¶</a></h3>
<p>JSON (JavaScript Object Notation) is an open standard file format, and data interchange format, that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and array data types (or any other serializable value.</p>
<p><a id='5'></a></p>
</div>
<div class="section" id="d-parquet-file">
<h3>1d. Parquet file<a class="headerlink" href="#d-parquet-file" title="Permalink to this headline">¶</a></h3>
<p>Apache Parquet is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem. It is similar to the other columnar-storage file formats available in Hadoop namely RCFile and ORC. It is compatible with most of the data processing frameworks in the Hadoop environment. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk.</p>
<p>Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types.  This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO.</p>
<p>Advantages of Storing Data in a Columnar Format:</p>
<ul class="simple">
<li><p>Columnar storage like Apache Parquet is designed to bring efficiency compared to row-based files like CSV. When querying columnar storage you can skip over the non-relevant data very quickly. As a result, aggregation queries are less time consuming compared to row-oriented databases. This way of storage has translated into hardware savings and minimized latency for accessing data.</p></li>
<li><p>Apache Parquet is built from the ground up. Hence it is able to support advanced nested data structures. The layout of Parquet data files is optimized for queries that process large volumes of data, in the gigabyte range for each individual file.</p></li>
<li><p>Parquet is built to support flexible compression options and efficient encoding schemes. As the data type for each column is quite similar, the compression of each column is straightforward (which makes queries even faster). Data can be compressed by using one of the several codecs available; as a result, different data files can be compressed differently.</p></li>
</ul>
<p>The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:</p>
<ul class="simple">
<li><p>Column-wise compression is efficient and saves storage space</p></li>
<li><p>Compression techniques specific to a type can be applied as the column values tend to be of the same type</p></li>
<li><p>Queries that fetch specific column values need not read the entire row data thus improving performance</p></li>
<li><p>Different encoding techniques can be applied to different columns</p></li>
</ul>
<p><a id='6'></a></p>
</div>
<div class="section" id="e-orc-file">
<h3>1e. ORC file<a class="headerlink" href="#e-orc-file" title="Permalink to this headline">¶</a></h3>
<p>Apache ORC (Optimized Row Columnar) is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem. It is similar to the other columnar-storage file formats available in the Hadoop ecosystem such as RCFile and Parquet. It is compatible with most of the data processing frameworks in the Hadoop environment.</p>
<p>ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. Storing data in a columnar format lets the reader read, decompress, and process only the values that are required for the current query. Because ORC files are type-aware, the writer chooses the most appropriate encoding for the type and builds an internal index as the file is written. Predicate pushdown uses those indexes to determine which stripes in a file need to be read for a particular query and the row indexes can narrow the search to a particular set of 10,000 rows. ORC supports the complete set of types in Hive, including the complex types: structs, lists, maps, and unions.</p>
<p>The ORC file format provides the following advantages:</p>
<ul class="simple">
<li><p>Efficient compression: Stored as columns and compressed, which leads to smaller disk reads. The columnar format is also ideal for vectorization optimizations in Tez.</p></li>
<li><p>Fast reads: ORC has a built-in index, min/max values, and other aggregates that cause entire stripes to be skipped during reads. In addition, predicate pushdown pushes filters into reads so that minimal rows are read. And Bloom filters further reduce the number of rows that are returned.</p></li>
<li><p>Proven in large-scale deployments: Facebook uses the ORC file format for a 300+ PB deployment.</p></li>
</ul>
<p><a id='7'></a></p>
</div>
<div class="section" id="e-avro-file">
<h3>1e. AVRO file<a class="headerlink" href="#e-avro-file" title="Permalink to this headline">¶</a></h3>
<p>Avro is a row-oriented remote procedure call and data serialization framework developed within Apache’s Hadoop project.</p>
<p>It uses JSON for defining data types and protocols, and serializes data in a compact binary format. Its primary use is in Apache Hadoop, where it can provide both a serialization format for persistent data, and a wire format for communication between Hadoop nodes, and from client programs to the Hadoop services.</p>
<p>Avro uses a schema to structure the data that is being encoded. It has two different types of schema languages; one for human editing (Avro IDL) and another which is more machine-readable based on JSON.</p>
<p>The Avro data source supports:</p>
<ul class="simple">
<li><p>Schema conversion: Automatic conversion between Apache Spark SQL and Avro records.</p></li>
<li><p>Partitioning: Easily reading and writing partitioned data without any extra configuration.</p></li>
<li><p>Compression: Compression to use when writing Avro out to disk. The supported types are uncompressed, snappy, and deflate.</p></li>
<li><p>Record names: Record name and namespace by passing a map of parameters with recordName and recordNamespace.</p></li>
</ul>
<p><a id='8'></a></p>
</div>
<div class="section" id="f-whole-binary-file">
<h3>1f. Whole Binary file<a class="headerlink" href="#f-whole-binary-file" title="Permalink to this headline">¶</a></h3>
<p>A binary file is a computer file that is not a text file.</p>
<p>Binary files are usually thought of as being a sequence of bytes, which means the binary digits (bits) are grouped in eights.</p>
<p>Binary files typically contain bytes that are intended to be interpreted as something other than text characters. Compiled computer programs are typical examples; indeed, compiled applications are sometimes referred to, particularly by programmers, as binaries. But binary files can also mean that they contain images, sounds, compressed versions of other files, etc. – in short, any type of file content whatsoever</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#./bin/spark-submit --packages org.apache.spark:spark-avro_2.12:3.0.2 ...</span>
<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Python Spark SQL basic example&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s1">&#39;spark.jars.packages&#39;</span><span class="p">,</span> <span class="s1">&#39;org.apache.spark:spark-avro_2.12:3.0.2&#39;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;John&quot;</span><span class="p">,</span><span class="mi">180</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="s2">&quot;1960-01-01&quot;</span><span class="p">,</span> <span class="s1">&#39;{“home”: 123456789, “office”:234567567}&#39;</span><span class="p">),])</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>        
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+---+----+---+----------+---------------------------------------+
|_1  |_2 |_3  |_4 |_5        |_6                                     |
+----+---+----+---+----------+---------------------------------------+
|John|180|true|1.7|1960-01-01|{“home”: 123456789, “office”:234567567}|
+----+---+----+---+----------+---------------------------------------+
</pre></div>
</div>
</div>
</div>
<p><a id='9'></a></p>
</div>
</div>
<div class="section" id="reading-writing-data-from-various-data-sources">
<h2>2. Reading &amp; Writing data from various data sources<a class="headerlink" href="#reading-writing-data-from-various-data-sources" title="Permalink to this headline">¶</a></h2>
<p><a id='10'></a></p>
<div class="section" id="a-reading-from-text-file-into-spark-data-frame">
<h3>2a. Reading from text file into spark data frame<a class="headerlink" href="#a-reading-from-text-file-into-spark-data-frame" title="Permalink to this headline">¶</a></h3>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>text(paths, wholetext=False, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None)</b>
Loads text files and returns a DataFrame whose schema starts with a string column named “value”, and followed by partitioned columns if there are any. The text files must be encoded as UTF-8.
By default, each line in the text file is a new row in the resulting DataFrame.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>paths – string, or list of strings, for input path(s).</p></li>
<li><p>wholetext – if true, read each file from input path(s) as a single row.</p></li>
<li><p>lineSep – defines the line separator that should be used for parsing.
If None is set, it covers all \r, \r\n and \n.</p></li>
<li><p>pathGlobFilter – an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.</p></li>
<li><p>recursiveFileLookup – recursively scan a directory for files. Using this option disables partition discovery.</p></li>
</ul>
</div>
<p><b>Input: Text File</b></p>
<div class="figure align-center">
<img alt="../_images/2a1.png" src="../_images/2a1.png" />
</div>
<p><b>Input: Spark data frame</b></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_text</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s2">&quot;data/text/sample.txt&quot;</span><span class="p">)</span>
<span class="n">df_text</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|value                                                                                                                                                                |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|Apache Spark is a unified analytics engine for large-scale data processing.                                                                                          |
|Apache Spark achieves high performance for both batch and streaming data, using a state-of-the-art DAG scheduler, a query optimizer, and a physical execution engine.|
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-the-spark-data-frame-content-into-a-text-file">
<h4>Saving the spark data frame content into a text file<a class="headerlink" href="#saving-the-spark-data-frame-content-into-a-text-file" title="Permalink to this headline">¶</a></h4>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>text(path, compression=None, lineSep=None)</b>
Saves the content of the DataFrame in a text file at the specified path. The text files will be encoded as UTF-8.
The DataFrame must have only one column that is of string type. Each row becomes a new line in the output file.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>path – the path in any Hadoop supported file system</p></li>
<li><p>compression – compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).</p></li>
<li><p>lineSep – defines the line separator that should be used for writing.
If None is set, it uses the default value, \n.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_text</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;overwrite&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s2">&quot;data/text/sample_write&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To verify the save, let us read the file into data frame</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="s2">&quot;data/text/sample_write&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|value                                                                                                                                                                |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|Apache Spark is a unified analytics engine for large-scale data processing.                                                                                          |
|Apache Spark achieves high performance for both batch and streaming data, using a state-of-the-art DAG scheduler, a query optimizer, and a physical execution engine.|
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</pre></div>
</div>
</div>
</div>
<p><a id='11'></a></p>
</div>
</div>
<div class="section" id="b-reading-from-csv-file-into-spark-data-frame">
<h3>2b. Reading from CSV file into spark data frame<a class="headerlink" href="#b-reading-from-csv-file-into-spark-data-frame" title="Permalink to this headline">¶</a></h3>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None)</b></p>
<p>Loads a CSV file and returns the result as a DataFrame.</p>
<p>This function will go through the input once to determine the input schema if inferSchema is enabled. To avoid going through the entire data once, disable inferSchema option or specify the schema explicitly using schema.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>path – string, or list of strings, for input path(s), or RDD of Strings storing CSV rows.</p></li>
<li><p>schema – an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).</p></li>
<li><p>sep – sets a separator (one or more characters) for each field and value. If None is set, it uses the default value, ,.</p></li>
<li><p>encoding – decodes the CSV files by the given encoding type. If None is set, it uses the default value, UTF-8.</p></li>
<li><p>quote – sets a single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, “. If you would like to turn off quotations, you need to set an empty string.</p></li>
<li><p>escape – sets a single character used for escaping quotes inside an already quoted value. If None is set, it uses the default value, .</p></li>
<li><p>comment – sets a single character used for skipping lines beginning with this character. By default (None), it is disabled.</p></li>
<li><p>header – uses the first line as names of columns. If None is set, it uses the default value, false. .. note:: if the given path is a RDD of Strings, this header option will remove all lines same with the header if exists.</p></li>
<li><p>inferSchema – infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false.</p></li>
<li><p>enforceSchema – If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files or the first header in RDD if the header option is set to true. Field names in the schema and column names in CSV headers are checked by their positions taking into account spark.sql.caseSensitive. If None is set, true is used by default. Though the default value is true, it is recommended to disable the enforceSchema option to avoid incorrect results.</p></li>
<li><p>ignoreLeadingWhiteSpace – A flag indicating whether or not leading whitespaces from values being read should be skipped. If None is set, it uses the default value, false.</p></li>
<li><p>ignoreTrailingWhiteSpace – A flag indicating whether or not trailing whitespaces from values being read should be skipped. If None is set, it uses the default value, false.</p></li>
<li><p>nullValue – sets the string representation of a null value. If None is set, it uses the default value, empty string. Since 2.0.1, this nullValue param applies to all supported types including the string type.</p></li>
<li><p>nanValue – sets the string representation of a non-number value. If None is set, it uses the default value, NaN.</p></li>
<li><p>positiveInf – sets the string representation of a positive infinity value. If None is set, it uses the default value, Inf.</p></li>
<li><p>negativeInf – sets the string representation of a negative infinity value. If None is set, it uses the default value, Inf.</p></li>
<li><p>dateFormat – sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.
timestampFormat – sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss[.SSS][XXX].</p></li>
<li><p>maxColumns – defines a hard limit of how many columns a record can have. If None is set, it uses the default value, 20480.</p></li>
<li><p>maxCharsPerColumn – defines the maximum number of characters allowed for any given value being read. If None is set, it uses the default value, -1 meaning unlimited length.</p></li>
<li><p>maxMalformedLogPerPartition – this parameter is no longer used since Spark 2.2.0. If specified, it is ignored.</p></li>
<li><p>mode –
allows a mode for dealing with corrupt records during parsing. If None is
set, it uses the default value, PERMISSIVE. Note that Spark tries to parse only required columns in CSV under column pruning. Therefore, corrupt records can be different based on required set of fields. This behavior can be controlled by spark.sql.csv.parser.columnPruning.enabled (enabled by default).
PERMISSIVE: when it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. A record with less/more tokens than schema is not a corrupted record to CSV. When it meets a record having fewer tokens than the length of the schema, sets null to extra fields. When the record has more tokens than the length of the schema, it drops extra tokens.
DROPMALFORMED: ignores the whole corrupted records.
FAILFAST: throws an exception when it meets corrupted records.
columnNameOfCorruptRecord – allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. If None is set, it uses the value specified in spark.sql.columnNameOfCorruptRecord.</p></li>
<li><p>multiLine – parse records, which may span multiple lines. If None is set, it uses the default value, false.
charToEscapeQuoteEscaping – sets a single character used for escaping the escape for the quote character. If None is set, the default value is escape character when escape and quote characters are different, \0 otherwise.</p></li>
<li><p>samplingRatio – defines fraction of rows used for schema inferring. If None is set, it uses the default value, 1.0.</p></li>
<li><p>emptyValue – sets the string representation of an empty value. If None is set, it uses the default value, empty string.</p></li>
<li><p>locale – sets a locale as language tag in IETF BCP 47 format. If None is set, it uses the default value, en-US. For instance, locale is used while parsing dates and timestamps.</p></li>
<li><p>lineSep – defines the line separator that should be used for parsing. If None is set, it covers all \r, \r\n and \n. Maximum length is 1 character.</p></li>
<li><p>pathGlobFilter – an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.</p></li>
<li><p>recursiveFileLookup – recursively scan a directory for files. Using this option disables partition discovery.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_csv</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s1">&#39;/Users/deepak/Documents/sparkbook/chapters/data/people.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_csv</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+
|name|weight|smoker|height|birthdate |phone_nos                            |favorite_colors    |address                                                          |
+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+
|john|180   |true  |1.7   |1960-01-01|{&quot;office&quot;:123456789,&quot;home&quot;:223456789}|[&quot;blue&quot;,&quot;red&quot;]     |{&quot;houseno&quot;:100,&quot;street&quot;:&quot;street1&quot;,&quot;city&quot;:&quot;city1&quot;,&quot;zipcode&quot;:12345}|
|tony|180   |true  |1.8   |1990-01-01|{&quot;office&quot;:223456789,&quot;home&quot;:323456789}|[&quot;green&quot;,&quot;purple&quot;] |{&quot;houseno&quot;:200,&quot;street&quot;:&quot;street2&quot;,&quot;city&quot;:&quot;city2&quot;,&quot;zipcode&quot;:22345}|
|mike|180   |true  |1.65  |1980-01-01|{&quot;office&quot;:323456789,&quot;home&quot;:423456789}|[&quot;yellow&quot;,&quot;orange&quot;]|{&quot;houseno&quot;:300,&quot;street&quot;:&quot;street3&quot;,&quot;city&quot;:&quot;city3&quot;,&quot;zipcode&quot;:32345}|
+----+------+------+------+----------+-------------------------------------+-------------------+-----------------------------------------------------------------+
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-the-spark-data-frame-content-into-a-csv-file">
<h4>Saving the spark data frame content into a CSV file<a class="headerlink" href="#saving-the-spark-data-frame-content-into-a-csv-file" title="Permalink to this headline">¶</a></h4>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>csv(path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None)</b>
Saves the content of the DataFrame in CSV format at the specified path.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>path – the path in any Hadoop supported file system</p></li>
<li><p>mode –
specifies the behavior of the save operation when data already exists.
append: Append contents of this DataFrame to existing data.
overwrite: Overwrite existing data.
ignore: Silently ignore this operation if data already exists.
error or errorifexists (default case): Throw an exception if data already
exists.</p></li>
<li><p>compression – compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).</p></li>
<li><p>sep – sets a separator (one or more characters) for each field and value. If None is set, it uses the default value, ,.</p></li>
<li><p>quote – sets a single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, “. If an empty string is set, it uses u0000 (null character).</p></li>
<li><p>escape – sets a single character used for escaping quotes inside an already quoted value. If None is set, it uses the default value, \</p></li>
<li><p>escapeQuotes – a flag indicating whether values containing quotes should always be enclosed in quotes. If None is set, it uses the default value true, escaping all values containing a quote character.</p></li>
<li><p>quoteAll – a flag indicating whether all values should always be enclosed in quotes. If None is set, it uses the default value false, only escaping values containing a quote character.</p></li>
<li><p>header – writes the names of columns as the first line. If None is set, it uses the default value, false.</p></li>
<li><p>nullValue – sets the string representation of a null value. If None is set, it uses the default value, empty string.</p></li>
<li><p>dateFormat – sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.</p></li>
<li><p>timestampFormat – sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss[.SSS][XXX].</p></li>
<li><p>ignoreLeadingWhiteSpace – a flag indicating whether or not leading whitespaces from values being written should be skipped. If None is set, it uses the default value, true.</p></li>
<li><p>ignoreTrailingWhiteSpace – a flag indicating whether or not trailing whitespaces from values being written should be skipped. If None is set, it uses the default value, true.</p></li>
<li><p>charToEscapeQuoteEscaping – sets a single character used for escaping the escape for the quote character. If None is set, the default value is escape character when escape and quote characters are different, \0 otherwise..</p></li>
<li><p>encoding – sets the encoding (charset) of saved csv files. If None is set, the default UTF-8 charset will be used.</p></li>
<li><p>emptyValue – sets the string representation of an empty value. If None is set, it uses the default value, “”.</p></li>
<li><p>lineSep – defines the line separator that should be used for writing. If None is set, it uses the default value, \n. Maximum length is 1 character.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_csv</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&quot;data/people_csv_write&quot;</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="s2">&quot;data/people_csv_write&quot;</span><span class="p">,</span><span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+--------------------+-------------------+--------------------+
|name|weight|smoker|height| birthdate|           phone_nos|    favorite_colors|             address|
+----+------+------+------+----------+--------------------+-------------------+--------------------+
|john|   180|  true|   1.7|1960-01-01|{&quot;office&quot;:1234567...|     [&quot;blue&quot;,&quot;red&quot;]|{&quot;houseno&quot;:100,&quot;s...|
|tony|   180|  true|   1.8|1990-01-01|{&quot;office&quot;:2234567...| [&quot;green&quot;,&quot;purple&quot;]|{&quot;houseno&quot;:200,&quot;s...|
|mike|   180|  true|  1.65|1980-01-01|{&quot;office&quot;:3234567...|[&quot;yellow&quot;,&quot;orange&quot;]|{&quot;houseno&quot;:300,&quot;s...|
+----+------+------+------+----------+--------------------+-------------------+--------------------+
</pre></div>
</div>
</div>
</div>
<p><a id='12'></a></p>
</div>
</div>
<div class="section" id="c-reading-from-json-file-into-spark-data-frame">
<h3>2c. Reading from JSON file into spark data frame<a class="headerlink" href="#c-reading-from-json-file-into-spark-data-frame" title="Permalink to this headline">¶</a></h3>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None, recursiveFileLookup=None)
</b>
Loads JSON files and returns the results as a DataFrame.
JSON Lines (newline-delimited JSON) is supported by default. For JSON (one record per file), set the multiLine parameter to true.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>path – string represents path to the JSON dataset, or a list of paths, or RDD of Strings storing JSON objects.</p></li>
<li><p>schema – an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).</p></li>
<li><p>primitivesAsString – infers all primitive values as a string type. If None is set, it uses the default value, false.</p></li>
<li><p>prefersDecimal – infers all floating-point values as a decimal type. If the values do not fit in decimal, then it infers them as doubles. If None is set, it uses the default value, false.</p></li>
<li><p>allowComments – ignores Java/C++ style comment in JSON records. If None is set, it uses the default value, false.</p></li>
<li><p>allowUnquotedFieldNames – allows unquoted JSON field names. If None is set, it uses the default value, false.</p></li>
<li><p>allowSingleQuotes – allows single quotes in addition to double quotes. If None is set, it uses the default value, true.</p></li>
<li><p>allowNumericLeadingZero – allows leading zeros in numbers (e.g. 00012). If None is set, it uses the default value, false.</p></li>
<li><p>allowBackslashEscapingAnyCharacter – allows accepting quoting of all character using backslash quoting mechanism. If None is set, it uses the default value, false.</p></li>
<li><p>mode –
allows a mode for dealing with corrupt records during parsing. If None is
set, it uses the default value, PERMISSIVE.
PERMISSIVE: when it meets a corrupted record, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a columnNameOfCorruptRecord field in an output schema.
DROPMALFORMED: ignores the whole corrupted records.
FAILFAST: throws an exception when it meets corrupted records.</p></li>
<li><p>columnNameOfCorruptRecord – allows renaming the new field having malformed string created by PERMISSIVE mode. This overrides spark.sql.columnNameOfCorruptRecord. If None is set, it uses the value specified in spark.sql.columnNameOfCorruptRecord.</p></li>
<li><p>dateFormat – sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.</p></li>
<li><p>timestampFormat – sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss[.SSS][XXX].</p></li>
<li><p>multiLine – parse one record, which may span multiple lines, per file. If None is set, it uses the default value, false.</p></li>
<li><p>allowUnquotedControlChars – allows JSON Strings to contain unquoted control characters (ASCII characters with value less than 32, including tab and line feed characters) or not.
encoding – allows to forcibly set one of standard basic or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. If None is set, the encoding of input JSON will be detected automatically when the multiLine option is set to true.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">jsonStrings</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;{&quot;name&quot;:&quot;Yin&quot;,&quot;age&quot;:45,&quot;smoker&quot;: true,&quot;test&quot;:34, &quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;},&quot;favorite_colors&quot;: [&quot;blue&quot;,&quot;green&quot;] }&#39;</span><span class="p">,]</span>
<span class="n">otherPeopleRDD</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">jsonStrings</span><span class="p">)</span>
<span class="n">otherPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">otherPeopleRDD</span><span class="p">)</span>
<span class="n">df_json</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;json&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;data/json&quot;</span><span class="p">)</span>
<span class="n">df_json</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----------------------------+----------+----------------+------+----+----------------------+------+------+
|address                     |birthdate |favorite_colors |height|name|phone_nos             |smoker|weight|
+----------------------------+----------+----------------+------+----+----------------------+------+------+
|[city1, 100, street1, 12345]|1960-01-01|[blue, red]     |1.7   |john|[223456789, 123456789]|true  |180   |
|[city2, 200, street2, 22345]|1990-01-01|[green, purple] |1.8   |tony|[323456789, 223456789]|true  |180   |
|[city3, 300, street3, 32345]|1980-01-01|[yellow, orange]|1.65  |mike|[423456789, 323456789]|true  |180   |
+----------------------------+----------+----------------+------+----+----------------------+------+------+
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-the-spark-data-frame-content-into-a-json-file">
<h4>Saving the spark data frame content into a JSON  file<a class="headerlink" href="#saving-the-spark-data-frame-content-into-a-json-file" title="Permalink to this headline">¶</a></h4>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>json(path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None)</b>
Saves the content of the DataFrame in JSON format (JSON Lines text format or newline-delimited JSON) at the specified path.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>path – the path in any Hadoop supported file system</p></li>
<li><p>mode –
specifies the behavior of the save operation when data already exists.
append: Append contents of this DataFrame to existing data.
overwrite: Overwrite existing data.
ignore: Silently ignore this operation if data already exists.
error or errorifexists (default case): Throw an exception if data already exists.</p></li>
<li><p>compression – compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, bzip2, gzip, lz4, snappy and deflate).</p></li>
<li><p>dateFormat – sets the string that indicates a date format. Custom date formats follow the formats at datetime pattern. This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.</p></li>
<li><p>timestampFormat – sets the string that indicates a timestamp format. Custom date formats follow the formats at datetime pattern. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss[.SSS][XXX].</p></li>
<li><p>encoding – specifies encoding (charset) of saved json files. If None is set, the default UTF-8 charset will be used.</p></li>
<li><p>lineSep – defines the line separator that should be used for writing. If None is set, it uses the default value, \n.</p></li>
<li><p>ignoreNullFields – Whether to ignore null fields when generating JSON objects. If None is set, it uses the default value, true.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_json</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;data/json_write&quot;</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;data/json_write&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+--------------------+----------+----------------+------+----+--------------------+------+------+
|             address| birthdate| favorite_colors|height|name|           phone_nos|smoker|weight|
+--------------------+----------+----------------+------+----+--------------------+------+------+
|[city1, 100, stre...|1960-01-01|     [blue, red]|   1.7|john|[223456789, 12345...|  true|   180|
|[city2, 200, stre...|1990-01-01| [green, purple]|   1.8|tony|[323456789, 22345...|  true|   180|
|[city3, 300, stre...|1980-01-01|[yellow, orange]|  1.65|mike|[423456789, 32345...|  true|   180|
+--------------------+----------+----------------+------+----+--------------------+------+------+
</pre></div>
</div>
</div>
</div>
<p><a id='13'></a></p>
</div>
</div>
<div class="section" id="d-reading-from-parquet-file-into-spark-data-frame">
<h3>2d. Reading from Parquet file into spark data frame<a class="headerlink" href="#d-reading-from-parquet-file-into-spark-data-frame" title="Permalink to this headline">¶</a></h3>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>parquet(*paths, **options)</b>
Loads Parquet files, returning the result as a DataFrame.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>mergeSchema – sets whether we should merge schemas collected from all Parquet part-files. This will override spark.sql.parquet.mergeSchema. The default value is specified in spark.sql.parquet.mergeSchema.</p></li>
<li><p>pathGlobFilter – an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.</p></li>
<li><p>recursiveFileLookup – recursively scan a directory for files. Using this option disables partition discovery. None is set, it uses the default value, \n.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_parquet</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/parquetfile&quot;</span><span class="p">)</span>
<span class="n">df_parquet</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|john|180   |true  |1.7   |1960-01-01|[office -&gt; 123456789, home -&gt; 223456789]|[blue, red]     |[100, street1, city1, 12345]|
|tony|180   |true  |1.8   |1990-01-01|[office -&gt; 223456789, home -&gt; 323456789]|[green, purple] |[200, street2, city2, 22345]|
|mike|180   |true  |1.65  |1980-01-01|[office -&gt; 323456789, home -&gt; 423456789]|[yellow, orange]|[300, street3, city3, 32345]|
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-the-spark-data-frame-content-into-a-parquet-file">
<h4>Saving the spark data frame content into a Parquet  file<a class="headerlink" href="#saving-the-spark-data-frame-content-into-a-parquet-file" title="Permalink to this headline">¶</a></h4>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>parquet(path, mode=None, partitionBy=None, compression=None)</b>
Saves the content of the DataFrame in Parquet format at the specified path.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>path – the path in any Hadoop supported file system</p></li>
<li><p>mode –
specifies the behavior of the save operation when data already exists.
append: Append contents of this DataFrame to existing data.
overwrite: Overwrite existing data.
ignore: Silently ignore this operation if data already exists.
error or errorifexists (default case): Throw an exception if data already exists.</p></li>
<li><p>partitionBy – names of partitioning columns</p></li>
<li><p>compression – compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, uncompressed, snappy, gzip, lzo, brotli, lz4, and zstd). This will override spark.sql.parquet.compression.codec. If None is set, it uses the value specified in spark.sql.parquet.compression.codec.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_parquet</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/parquetfile_write&quot;</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/parquetfile_write&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|john|180   |true  |1.7   |1960-01-01|[office -&gt; 123456789, home -&gt; 223456789]|[blue, red]     |[100, street1, city1, 12345]|
|tony|180   |true  |1.8   |1990-01-01|[office -&gt; 223456789, home -&gt; 323456789]|[green, purple] |[200, street2, city2, 22345]|
|mike|180   |true  |1.65  |1980-01-01|[office -&gt; 323456789, home -&gt; 423456789]|[yellow, orange]|[300, street3, city3, 32345]|
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
</pre></div>
</div>
</div>
</div>
<p><a id='14'></a></p>
</div>
</div>
<div class="section" id="e-reading-from-an-orc-file-into-spark-data-frame">
<h3>2e. Reading from an ORC file into spark data frame<a class="headerlink" href="#e-reading-from-an-orc-file-into-spark-data-frame" title="Permalink to this headline">¶</a></h3>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>orc(path, mergeSchema=None, pathGlobFilter=None, recursiveFileLookup=None)</b>
Loads ORC files, returning the result as a DataFrame.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>mergeSchema – sets whether we should merge schemas collected from all ORC part-files. This will override spark.sql.orc.mergeSchema. The default value is specified in spark.sql.orc.mergeSchema.</p></li>
<li><p>pathGlobFilter – an optional glob pattern to only include files with paths matching the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter. It does not change the behavior of partition discovery.</p></li>
<li><p>recursiveFileLookup – recursively scan a directory for files. Using this option disables partition discovery.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_orc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s2">&quot;data/orcfile&quot;</span><span class="p">)</span>
<span class="n">df_orc</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|john|180   |true  |1.7   |1960-01-01|[home -&gt; 223456789, office -&gt; 123456789]|[blue, red]     |[100, street1, city1, 12345]|
|tony|180   |true  |1.8   |1990-01-01|[home -&gt; 323456789, office -&gt; 223456789]|[green, purple] |[200, street2, city2, 22345]|
|mike|180   |true  |1.65  |1980-01-01|[home -&gt; 423456789, office -&gt; 323456789]|[yellow, orange]|[300, street3, city3, 32345]|
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-the-spark-data-frame-content-into-an-orc-file">
<h4>Saving the spark data frame content into an ORC  file<a class="headerlink" href="#saving-the-spark-data-frame-content-into-an-orc-file" title="Permalink to this headline">¶</a></h4>
<div class="admonition-syntax admonition">
<p class="admonition-title">Syntax</p>
<p><b>orc(path, mode=None, partitionBy=None, compression=None)</b>
Saves the content of the DataFrame in ORC format at the specified path.</p>
<p><b>Parameters</b>:</p>
<ul class="simple">
<li><p>path – the path in any Hadoop supported file system</p></li>
<li><p>mode –
specifies the behavior of the save operation when data already exists.
append: Append contents of this DataFrame to existing data.
overwrite: Overwrite existing data.
ignore: Silently ignore this operation if data already exists.
error or errorifexists (default case): Throw an exception if data already exists.</p></li>
<li><p>partitionBy – names of partitioning columns</p></li>
<li><p>compression – compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, snappy, zlib, and lzo). This will override orc.compress and spark.sql.orc.compression.codec. If None is set, it uses the value specified in spark.sql.orc.compression.codec.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_orc</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s2">&quot;data/orcfile_write&quot;</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">orc</span><span class="p">(</span><span class="s2">&quot;data/orcfile_write&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|john|180   |true  |1.7   |1960-01-01|[home -&gt; 223456789, office -&gt; 123456789]|[blue, red]     |[100, street1, city1, 12345]|
|tony|180   |true  |1.8   |1990-01-01|[home -&gt; 323456789, office -&gt; 223456789]|[green, purple] |[200, street2, city2, 22345]|
|mike|180   |true  |1.65  |1980-01-01|[home -&gt; 423456789, office -&gt; 323456789]|[yellow, orange]|[300, street3, city3, 32345]|
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
</pre></div>
</div>
</div>
</div>
<p><a id='15'></a></p>
</div>
</div>
<div class="section" id="f-reading-from-an-avro-file-into-spark-data-frame">
<h3>2f. Reading from an AVRO file into spark data frame<a class="headerlink" href="#f-reading-from-an-avro-file-into-spark-data-frame" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!./bin/spark-submit --packages org.apache.spark:spark-avro_2.12:3.0.2 ...</span>
<span class="c1">##.config(&#39;spark.jars&#39;, &#39;org.apache.spark:spark-avro_2.12:3.0.2&#39;)\</span>
<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Python Spark SQL basic example&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s1">&#39;spark.jars.packages&#39;</span><span class="p">,</span> <span class="s1">&#39;org.apache.spark:spark-avro_2.12:3.0.2&#39;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;John&quot;</span><span class="p">,</span><span class="mi">180</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="s2">&quot;1960-01-01&quot;</span><span class="p">,</span> <span class="s1">&#39;{“home”: 123456789, “office”:234567567}&#39;</span><span class="p">),])</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>        
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+---+----+---+----------+---------------------------------------+
|_1  |_2 |_3  |_4 |_5        |_6                                     |
+----+---+----+---+----------+---------------------------------------+
|John|180|true|1.7|1960-01-01|{“home”: 123456789, “office”:234567567}|
+----+---+----+---+----------+---------------------------------------+
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_avro</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;avro&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;data/avrofile&quot;</span><span class="p">)</span>
<span class="n">df_avro</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|john|180   |true  |1.7   |1960-01-01|[office -&gt; 123456789, home -&gt; 223456789]|[blue, red]     |[100, street1, city1, 12345]|
|tony|180   |true  |1.8   |1990-01-01|[office -&gt; 223456789, home -&gt; 323456789]|[green, purple] |[200, street2, city2, 22345]|
|mike|180   |true  |1.65  |1980-01-01|[office -&gt; 323456789, home -&gt; 423456789]|[yellow, orange]|[300, street3, city3, 32345]|
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
</pre></div>
</div>
</div>
</div>
<div class="section" id="saving-the-spark-data-frame-content-into-an-avro-file">
<h4>Saving the spark data frame content into an AVRO  file<a class="headerlink" href="#saving-the-spark-data-frame-content-into-an-avro-file" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_avro</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;avro&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;data/avrofile_write&quot;</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;avro&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;data/avrofile_write&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|name|weight|smoker|height|birthdate |phone_nos                               |favorite_colors |address                     |
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
|john|180   |true  |1.7   |1960-01-01|[office -&gt; 123456789, home -&gt; 223456789]|[blue, red]     |[100, street1, city1, 12345]|
|tony|180   |true  |1.8   |1990-01-01|[office -&gt; 223456789, home -&gt; 323456789]|[green, purple] |[200, street2, city2, 22345]|
|mike|180   |true  |1.65  |1980-01-01|[office -&gt; 323456789, home -&gt; 423456789]|[yellow, orange]|[300, street3, city3, 32345]|
+----+------+------+------+----------+----------------------------------------+----------------+----------------------------+
</pre></div>
</div>
</div>
</div>
<p><a id='16'></a></p>
</div>
</div>
<div class="section" id="g-reading-from-an-whole-binary-file-into-spark-data-frame">
<h3>2g. Reading from an Whole Binary file into spark data frame<a class="headerlink" href="#g-reading-from-an-whole-binary-file-into-spark-data-frame" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;binaryFile&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;../images/banner.png&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+--------------------+-------------------+------+--------------------+
|                path|   modificationTime|length|             content|
+--------------------+-------------------+------+--------------------+
|file:/Users/deepa...|2021-02-20 22:40:59|339845|[89 50 4E 47 0D 0...|
+--------------------+-------------------+------+--------------------+
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ngdeepak/testbook",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="chapter2-dataframe.html" title="previous page">Chapter 2 : DataFrames</a>
    <a class='right-next' id="next-link" href="chapter4-data-types-schema.html" title="next page">Chapter 4 : Data Types &amp; Schema</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Deepak Gowda<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>